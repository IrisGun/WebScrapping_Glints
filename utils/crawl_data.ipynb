{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from urllib import request\n",
    "import datetime\n",
    "import requests\n",
    "from datetime import datetime \n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Setting browser's options\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--ignore-certificate-errors')\n",
    "options.add_argument(\"--start-maximized\")\n",
    "options.add_argument(\"--disable-popup-blocking\")\n",
    "\n",
    "# Path to the installed compatible chromedriver\n",
    "path = 'Your_chromedriver_path'\n",
    "service = Service(executable_path=path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "\n",
    "# Access to Glints website\n",
    "driver.get('https://glints.com/vn/opportunities/jobs/explore')\n",
    "print('-- Glints accessed')\n",
    "\n",
    "# Wait for 2 second for the site to load\n",
    "sleep(2)\n",
    "# Get html content\n",
    "res = requests.get(driver.current_url)\n",
    "soup2 = BeautifulSoup(res.text, 'html.parser')\n",
    "page_source = BeautifulSoup(driver.page_source)\n",
    "\n",
    "\n",
    "\n",
    "# Close pop-up. Pop-up will appear only if the page was scrolled down (took a long time to figure out tho)\n",
    "html = driver.find_element(By.TAG_NAME, 'html')\n",
    "html.send_keys(Keys.DOWN)\n",
    "sleep(1)\n",
    "html.send_keys(Keys.ESCAPE)\n",
    "html.send_keys(Keys.UP)\n",
    "driver.implicitly_wait(1)\n",
    "\n",
    "\n",
    "#Send job search keyword:\n",
    "#key_word = input('Input keyword: ') \n",
    "keyword = 'Data Analyst'\n",
    "job_input = driver.find_element(By.XPATH, '//*[@id=\"__next\"]/div/div[3]/div[2]/div[2]/div[2]/div[2]/div/div/div[1]/div/input')\n",
    "sleep(1)\n",
    "job_input.send_keys(Keys.CONTROL + \"a\")\n",
    "job_input.send_keys(Keys.DELETE)\n",
    "job_input.send_keys(keyword)\n",
    "job_input.send_keys(Keys.RETURN)\n",
    "\n",
    "#Click search\n",
    "search_button = driver.find_element(By.XPATH, '//*[@id=\"__next\"]/div/div[3]/div[2]/div[2]/div[2]/div[2]/div/div/div[3]/button')\n",
    "search_button.send_keys(Keys.ENTER)\n",
    "print('-- Start searching...')\n",
    "\n",
    "#Infinite scroll so instead of define Next button\n",
    "#But scroll fairly to the bottom and wait until the very last page loaded\n",
    "while True:\n",
    "    page_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    target_height = page_height - 1220\n",
    "    driver.execute_script(\"window.scrollTo(0, %s);\" %target_height )\n",
    "    sleep(2)\n",
    "    try:\n",
    "        state = driver.find_element(By.XPATH, '//*[@id=\"__next\"]/div/div[3]/div[2]/div[2]/div[2]/div[4]/div[2]/div[2]/span')\n",
    "        if state.text == 'Đã tải lên tất cả cơ hội việc làm': \n",
    "            print('-- All page loaded') \n",
    "            break\n",
    "    except:\n",
    "        continue \n",
    "        \n",
    "job_id = BeautifulSoup(driver.page_source).find_all('a', class_='CompactOpportunityCardsc__CardAnchorWrapper-sc-1y4v110-18 iOjUdU job-search-results_job-card_link')\n",
    "print(\"-- Found {} jobs.\".format(len(job_id)))\n",
    "print(\"-- Start crawling job's detailed information\")\n",
    "\n",
    "\n",
    "#Get link for each job\n",
    "job_url = []\n",
    "for i in job_id:\n",
    "    link = i.get('href')\n",
    "    url = 'https://glints.com'+link\n",
    "    job_url.append(url)\n",
    "\n",
    "\n",
    "#Export job_url list to file in order to prevent data loss due to sudden termination of the program\n",
    "today = datetime.today().strftime('%y%m%d')\n",
    "with open(f'Glints_joburl_{today}.txt', 'w') as f:\n",
    "    for line in job_url:\n",
    "        f.write(\"%s\\n\" % line)\n",
    "\n",
    "\n",
    "#Define the DataFrame structure\n",
    "df = pd.DataFrame(columns = [\"job_title\", 'job_link', \"company_name\", \"company_link\", \"salary\", \"currency\",\n",
    "                           \"category\", \"contract\", \"experience\", \"benefit\", \"requirement\",'industry',\n",
    "                           \"company_size\", \"company_location\", \"job_location\", \"posted\", \"updated\"])\n",
    "# Add job_url to the DataFrame\n",
    "df['job_link'] = job_url\n",
    "\n",
    "# Define a dict which will be appended to the df after each link being crawled\n",
    "crawl_set = {}\n",
    "\n",
    "\n",
    "for link in job_url:\n",
    "    driver.get(link)\n",
    "#     sleep(3)\n",
    "    try:\n",
    "        element = WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div[class=\"GlintsContainer-sc-ap1z3q-0 iUnyrV\"]')))\n",
    "        html_of_interest = driver.execute_script('return arguments[0].innerHTML',element)\n",
    "        soup = BeautifulSoup(html_of_interest, 'lxml')\n",
    "    except:\n",
    "        print('Error orcurred at url: {}'.format(link))\n",
    "        continue\n",
    "        \n",
    "    #Job title\n",
    "    try:\n",
    "        crawl_set['job_title'] = soup.select('div[class=\"TopFoldsc__JobOverviewHeader-sc-kklg8i-24 gfOGEj\"]')[0].text\n",
    "    except:\n",
    "        crawl_set['job_title'] = None\n",
    "        \n",
    "    #Company name\n",
    "    try:\n",
    "        crawl_set['company_name'] = soup.select('div[class =\"TopFoldsc__JobOverViewCompanyName-sc-kklg8i-5 eLQvRY\"]>a')[0].text\n",
    "    except:\n",
    "        crawl_set['company_name'] = None\n",
    "        \n",
    "    #Company profile\n",
    "    try:\n",
    "        crawl_set['company_link'] = driver.find_element(By.XPATH, '//*[@id=\"__next\"]/div/div[3]/div[2]/div[2]/div[2]/div/main/div[1]/div[2]/div/div[2]/div/a').get_attribute('href')\n",
    "    except:\n",
    "        crawl_set['company_link'] = None\n",
    "        \n",
    "    #salary\n",
    "    try:\n",
    "        crawl_set['salary'] = soup.select('span[class=\"TopFoldsc__BasicSalary-sc-kklg8i-15 lolAnb\"]')[0].text\n",
    "        ##soup.select('.TopFoldsc__JobOverViewInfo-sc-kklg8i-9.EWOdY')[0].text\n",
    "    except:\n",
    "        crawl_set['salary'] = None\n",
    "        \n",
    "    #currency \n",
    "    try:\n",
    "        crawl_set['currency'] = soup.select('span[class=\"TopFoldsc__CurrencyCode-sc-kklg8i-30 eCrNiw\"]')[0].text\n",
    "    except:\n",
    "        crawl_set['currency'] = None\n",
    "        \n",
    "    #category\n",
    "    try:\n",
    "        crawl_set['category'] = soup.select('div[class=\"TopFoldsc__JobOverViewInfo-sc-kklg8i-9 EWOdY\"]>a')[0].text\n",
    "        ##soup.select('.TopFoldsc__JobOverViewInfo-sc-kklg8i-9.EWOdY')[1].text\n",
    "    except:\n",
    "        crawl_set['category'] = None\n",
    "        \n",
    "    #contract\n",
    "    try:\n",
    "        crawl_set['contract'] = soup.select('div[class=\"TopFoldsc__JobOverViewInfo-sc-kklg8i-9 EWOdY\"]')[1].text\n",
    "        ##soup.select('.TopFoldsc__JobOverViewInfo-sc-kklg8i-9.EWOdY')[2].text\n",
    "    except:\n",
    "        crawl_set['contract'] = None\n",
    "\n",
    "    #experience\n",
    "    try:\n",
    "        crawl_set['experience'] = soup.select('div[class=\"TopFoldsc__JobOverViewInfo-sc-kklg8i-9 EWOdY\"]')[2].text\n",
    "        ##soup.select('.TopFoldsc__JobOverViewInfo-sc-kklg8i-9.EWOdY')[3].text\n",
    "    except:\n",
    "        crawl_set['experience'] = None\n",
    "\n",
    "    #requirement\n",
    "    requirement = soup.select('.TagStyle__TagContent-sc-66xi2f-0.bxpfKm.tag-content')\n",
    "    req_list = []\n",
    "    for i in range(len(requirement)):\n",
    "        req_list.append(requirement[i].text)\n",
    "    crawl_set['requirement'] = ','.join(req_list)\n",
    "\n",
    "    #benefit packages\n",
    "    try:\n",
    "        benefit = soup.select('ul[class=\"Benefitssc__BenefitList-sc-10xec8z-1 iCclQu\"]>li>div>h3')\n",
    "        benefit_list = []\n",
    "        if len(benefit_list)!=0:\n",
    "            for i in benefit:\n",
    "                benefit_list.append(i.text)\n",
    "            crawl_set['benefit'] = ','.join(benefit_list)\n",
    "        else:\n",
    "            crawl_set['benefit'] = None\n",
    "    except:\n",
    "        crawl_set['benefit'] = None\n",
    "\n",
    "\n",
    "    #industry\n",
    "    try:\n",
    "        crawl_set['industry'] = soup.select('div[class=\"AboutCompanySectionsc__CompanyIndustryAndSize-sc-7g2mk6-7 iGZjWK\"]>span')[0].text\n",
    "    except:\n",
    "        crawl_set['industry'] = None\n",
    "\n",
    "\n",
    "    #Company size\n",
    "    try:\n",
    "        crawl_set['company_size'] = soup.select('div[class=\"AboutCompanySectionsc__CompanyIndustryAndSize-sc-7g2mk6-7 iGZjWK\"]>span')[1].text\n",
    "    except:\n",
    "        crawl_set['company_size'] = None\n",
    "\n",
    "    #company_location\n",
    "    try:\n",
    "        crawl_set['company_location'] = soup.select('div[class=\"AboutCompanySectionsc__AddressWrapper-sc-7g2mk6-14 bBEGUc\"]')[0].text\n",
    "    except:\n",
    "        crawl_set['company_location'] = None\n",
    "\n",
    "    #job location\n",
    "    try:\n",
    "        crawl_set['job_location'] = driver.find_element(By.XPATH, '//*[@id=\"__next\"]/div/div[3]/div[2]/div[2]/div[1]/div/label[3]/a').text.lstrip('Việc Làm Tại')\n",
    "    except:\n",
    "        crawl_set['job_location'] = None\n",
    "\n",
    "    #post\n",
    "    try:\n",
    "        posted = soup.select('span[class=\"TopFoldsc__PostedAt-sc-kklg8i-13 vnaHT\"]')\n",
    "        posted = posted[0].text.split(' ',1)[1]\n",
    "        crawl_set['posted'] = posted\n",
    "    except:\n",
    "        crawl_set['posted'] = None\n",
    "\n",
    "    #update\n",
    "    try:\n",
    "        updated = soup.select('span[class=\"TopFoldsc__UpdatedAt-sc-kklg8i-14 kjxTBC\"]')\n",
    "        updated = updated[0].text.split(' ',1)[1]\n",
    "        crawl_set['updated'] = updated\n",
    "    except:\n",
    "        crawl_set['updated'] = None\n",
    "        \n",
    "    #job_link\n",
    "    try:\n",
    "        crawl_set['job_link'] = link\n",
    "    except:\n",
    "        crawl_set['job_link'] = 'Error'\n",
    "\n",
    "    #crawled time\n",
    "    crawl_set['scrapped_time'] = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    df=df.append(crawl_set, ignore_index = True)\n",
    "    \n",
    "    if len(df) == len(job_url):\n",
    "        print('--Finish scrapping')\n",
    "    elif len(df)%20 == 0:\n",
    "        print('--There are {} links scraped as {}% of total. {} to go. Continue...'.format(len(df), round(len(df)/len(job_url)*100,2), len(job_url)-len(df)))\n",
    "\n",
    "\n",
    "# Export crawl data to csv file\n",
    "df.to_csv(f\"data/Glintsdata_{today}.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
